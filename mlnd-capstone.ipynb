{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Nanodegree - Capstone Project\n",
    "\n",
    "## Distracted Driver Detection\n",
    "\n",
    "## Project: Write a program to detect distracted drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import applications\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing import image                  \n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt   \n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.datasets import load_files       \n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driving_class_to_description = {\n",
    "    \"c0\": \"safe driving\",\n",
    "    \"c1\": \"texting - right\",\n",
    "    \"c2\": \"talking on the phone - right\",\n",
    "    \"c3\": \"texting - left\",\n",
    "    \"c4\": \"talking on the phone - left\",\n",
    "    \"c5\": \"operating the radio\",\n",
    "    \"c6\": \"drinking\",\n",
    "    \"c7\": \"reaching behind\",\n",
    "    \"c8\": \"hair and makeup\",\n",
    "    \"c9\": \"talking to passenger\",\n",
    "    0: \"safe driving\",\n",
    "    1: \"texting - right\",\n",
    "    2: \"talking on the phone - right\",\n",
    "    3: \"texting - left\",\n",
    "    4: \"talking on the phone - left\",\n",
    "    5: \"operating the radio\",\n",
    "    6: \"drinking\",\n",
    "    7: \"reaching behind\",\n",
    "    8: \"hair and makeup\",\n",
    "    9: \"talking to passenger\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    driver_files = np.array(data['filenames'])\n",
    "    driver_targets = np_utils.to_categorical(np.array(data['target']), 10)\n",
    "    # split into test/train here?\n",
    "    return driver_files, driver_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "all_files, all_targets = load_dataset('StateFarm/imgs/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df = pd.read_csv('StateFarm/driver_imgs_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = file_df.subject.unique()\n",
    "print('There are %d subjects with data'% len(subjects))\n",
    "\n",
    "train_subjects = subjects[:-4]\n",
    "validate_subjects = subjects[-4:-3]\n",
    "test_subjects = subjects[-3:]\n",
    "print('Training subjects:')\n",
    "print(train_subjects)\n",
    "print('Validate subjects:')\n",
    "print(validate_subjects)\n",
    "print('Test subjects:')\n",
    "print(test_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rows = file_df.loc[file_df['subject'].isin(test_subjects)]\n",
    "validate_rows = file_df.loc[file_df['subject'].isin(validate_subjects)]\n",
    "train_rows = file_df.loc[file_df['subject'].isin(train_subjects)]\n",
    "\n",
    "train_files = train_rows.apply(lambda x: \"{}/{}\".format(x[1],x[2]), axis=1)\n",
    "validate_files = validate_rows.apply(lambda x: \"{}/{}\".format(x[1],x[2]), axis=1)\n",
    "test_files = test_rows.apply(lambda x: \"{}/{}\".format(x[1],x[2]), axis=1)\n",
    "\n",
    "print(\"train_files count: {}\".format(len(train_files)))\n",
    "print(\"validate_files count: {}\".format(len(validate_files)))\n",
    "print(\"test_files count: {}\".format(len(test_files)))\n",
    "\n",
    "train_classes = train_files.apply(lambda x: x.split('/')[0].split('c')[1])\n",
    "train_classes_categories = np_utils.to_categorical(train_classes)\n",
    "validate_classes = validate_files.apply(lambda x: x.split('/')[0].split('c')[1])\n",
    "validate_classes_categories = np_utils.to_categorical(validate_classes)\n",
    "test_classes = test_files.apply(lambda x: x.split('/')[0].split('c')[1])\n",
    "test_classes_categories = np_utils.to_categorical(test_classes)\n",
    "\n",
    "assert len(train_classes) == len(train_files)\n",
    "assert len(train_classes) == len(train_classes_categories)\n",
    "\n",
    "assert len(validate_classes) == len(validate_files)\n",
    "assert len(validate_classes) == len(validate_classes_categories)\n",
    "\n",
    "assert len(test_classes) == len(test_files)\n",
    "assert len(test_classes) == len(test_classes_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(\"{}/{}\".format('StateFarm/imgs/train', img_path), target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "validate_tensors = paths_to_tensor(validate_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255\n",
    "\n",
    "train_classes_categories = train_classes_categories\n",
    "validate_classes_categories = validate_classes_categories\n",
    "test_classes_categories = test_classes_categories\n",
    "\n",
    "# train_tensors = paths_to_tensor(train_files[:1000]).astype('float32')/255\n",
    "# validate_tensors = paths_to_tensor(validate_files[:100]).astype('float32')/255\n",
    "# test_tensors = paths_to_tensor(test_files[:100]).astype('float32')/255\n",
    "\n",
    "# train_classes_categories = train_classes_categories[:1000]\n",
    "# validate_classes_categories = validate_classes_categories[:100]\n",
    "# test_classes_categories = test_classes_categories[:100]\n",
    "\n",
    "assert len(train_files) == len(train_tensors)\n",
    "assert len(validate_files) == len(validate_tensors)\n",
    "assert len(test_files) == len(test_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.argmax(train_classes_categories, axis=1))\n",
    "plt.title('Training Class Distribution')\n",
    "plt.xlabel('Image Count')\n",
    "plt.ylabel('Driving Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50, 50))  # width, height in inches\n",
    "dim = 4\n",
    "for target_class in range(10):\n",
    "    c = np.argmax(train_classes_categories, axis=1)\n",
    "    idx = np.where(c == target_class)[0][0]\n",
    "    tensor = train_tensors[idx]\n",
    "    sub = fig.add_subplot(dim, dim, target_class + 1)\n",
    "    sub.imshow(tensor, interpolation='nearest')\n",
    "    text_params = {'fontweight': 'bold'}\n",
    "\n",
    "    sub.text(0,\n",
    "             10,\n",
    "             \"actual: {}\".format(driving_class_to_description[target_class]),\n",
    "             color='g',\n",
    "             size=20,\n",
    "             bbox=dict(boxstyle=\"square\", ec=(1., 0.5, 0.5), fc=(1., 0.8, 0.8), ),\n",
    "             **text_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8\n",
    "# https://www.safaribooksonline.com/library/view/python-deep-learning/9781787125193/a0b05e70-9f53-404c-a975-09ac766389a1.xhtml\n",
    "# https://alexisbcook.github.io/2017/using-transfer-learning-to-classify-images-with-keras/\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "# train_data_dir = \"data/train\"\n",
    "# validation_data_dir = \"data/val\"\n",
    "nb_train_samples = len(train_tensors)\n",
    "nb_validation_samples = len(validate_tensors) \n",
    "batch_size = 20\n",
    "epochs = 50\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "model = applications.ResNet50(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chop_layers = 0\n",
    "for layer in model.layers[:-chop_layers]:\n",
    "    layer.trainable = False\n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "# creating the final model \n",
    "model_final = Model(input = model.input, output = predictions)\n",
    "model_final.summary()\n",
    "# compile the model \n",
    "# opt = Adam()\n",
    "# model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.Adam(), metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.layers))\n",
    "print(len(model.layers[:-chop_layers]))\n",
    "print(nb_train_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"resnet50_1.h5\", \n",
    "                             monitor='val_acc',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=False, \n",
    "                             mode='auto', \n",
    "                             period=1)\n",
    "early = EarlyStopping(monitor='val_acc', \n",
    "                      min_delta=0, \n",
    "                      patience=10, \n",
    "                      verbose=1, \n",
    "                      mode='auto')\n",
    "\n",
    "\n",
    "# Train the model \n",
    "# model_final.fit_generator(train_tensors,\n",
    "#                           samples_per_epoch = nb_train_samples,\n",
    "#                           # steps_per_epoch = 1,\n",
    "#                           epochs = epochs,\n",
    "#                           validation_data = validate_tensors,\n",
    "#                           nb_val_samples = nb_validation_samples,\n",
    "#                           callbacks = [checkpoint, early])\n",
    "# model.fit(train_Resnet50, train_classes_categories, \n",
    "#                    validation_data=(valid_Resnet50, validate_classes_categories),\n",
    "#                    epochs=1, batch_size=20, callbacks=[checkpointer_Resnet50], verbose=1)\n",
    "\n",
    "history = model_final.fit(x=train_tensors,\n",
    "                          y=train_classes_categories,\n",
    "                          # samples_per_epoch=nb_train_samples,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          verbose=1,\n",
    "                          validation_data=(validate_tensors, validate_classes_categories),\n",
    "                          callbacks = [checkpoint, early]\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(history.history['acc'])), history.history['acc'], label='training')\n",
    "plt.plot(np.arange(len(history.history['val_acc'])), history.history['val_acc'], label='validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy ')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "plt.plot(np.arange(len(history.history['loss'])), history.history['loss'], label='loss')\n",
    "plt.plot(np.arange(len(history.history['val_loss'])), history.history['val_loss'], label='val_loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss ')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model_final.predict(x=test_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_classes = np.argmax(test_classes_categories, axis=1)\n",
    "predicted_classes = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "test_accuracy = 100*np.sum(actual_classes==predicted_classes)/len(test_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mispredicts\n",
    "\n",
    "# for i in range(len(test_tensors)):\n",
    "#     if predicted_classes[i] == actual_classes[i]:\n",
    "#         print(\"{} matched '{}'\".format(i, driving_class_to_description[predicted_classes[i]]))\n",
    "#     if predicted_classes[i] != actual_classes[i]:\n",
    "#         print(\"{} predicted '{}' != actual '{}'\".format(i, driving_class_to_description[predicted_classes[i]], driving_class_to_description[actual_classes[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50, 50))  # width, height in inches\n",
    "j = 0\n",
    "dim = 4\n",
    "for i in shuffle(range(len(test_tensors))):\n",
    "    tensor = test_tensors[i]\n",
    "    if predicted_classes[i] != actual_classes[i]:\n",
    "        sub = fig.add_subplot(dim, dim, j + 1)\n",
    "        sub.imshow(tensor, interpolation='nearest')\n",
    "        text_params = {'fontweight': 'bold'}\n",
    "        sub.text(0,\n",
    "                 10,\n",
    "                 \"pred: {}\".format(driving_class_to_description[predicted_classes[i]]),\n",
    "                 color='r',\n",
    "                 size=20,\n",
    "                 bbox=dict(boxstyle=\"square\", ec=(1., 0.5, 0.5), fc=(1., 0.8, 0.8), ),\n",
    "                 **text_params)\n",
    "        sub.text(0, \n",
    "                 25, \n",
    "                 \"actual: {}\".format(driving_class_to_description[actual_classes[i]]), \n",
    "                 color='g',\n",
    "                 size=20,\n",
    "                 bbox=dict(boxstyle=\"square\", ec=(1., 0.5, 0.5), fc=(1., 0.8, 0.8), ),\n",
    "                 **text_params)\n",
    "\n",
    "        j = j + 1\n",
    "        if j == dim * dim:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(actual_classes, predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(actual_classes, predicted_classes)\n",
    "plt.figure(figsize = (10,7))\n",
    "hm = sns.heatmap(confusion, annot=True, fmt=\"d\")\n",
    "hm.set_ylabel('True label')\n",
    "hm.set_xlabel('Predicted label')\n",
    "hm.set_title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Problem statement\n",
    "* Image samples on grid\n",
    "* Learning Curve Graphs\n",
    "* Architecture\n",
    "    * How many layers to remove\n",
    "    * How many additional layers to ad\n",
    "* Hyperparameter Tuning\n",
    "* Training parameters\n",
    "    * Optimizer\n",
    "    * Loss function\n",
    "    * Metrics\n",
    "* Analysis\n",
    "    * Baseline\n",
    "    * Printing out errors\n",
    "* Testing on other images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=m5RjXjvAAhQ&feature=youtu.be"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
