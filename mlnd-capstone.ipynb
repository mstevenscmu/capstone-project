{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Nanodegree - Capstone Project\n",
    "\n",
    "## Distracted Driver Detection\n",
    "\n",
    "## Project: Write a program to detect distracted drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import applications\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing import image                  \n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt   \n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.datasets import load_files       \n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, log_loss\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary is used to map class short names to their full\n",
    "# description when generating graphs and charts\n",
    "driving_class_to_description = {\n",
    "    \"c0\": \"safe driving\",\n",
    "    \"c1\": \"texting - right\",\n",
    "    \"c2\": \"talking on the phone - right\",\n",
    "    \"c3\": \"texting - left\",\n",
    "    \"c4\": \"talking on the phone - left\",\n",
    "    \"c5\": \"operating the radio\",\n",
    "    \"c6\": \"drinking\",\n",
    "    \"c7\": \"reaching behind\",\n",
    "    \"c8\": \"hair and makeup\",\n",
    "    \"c9\": \"talking to passenger\",\n",
    "    0: \"safe driving\",\n",
    "    1: \"texting - right\",\n",
    "    2: \"talking on the phone - right\",\n",
    "    3: \"texting - left\",\n",
    "    4: \"talking on the phone - left\",\n",
    "    5: \"operating the radio\",\n",
    "    6: \"drinking\",\n",
    "    7: \"reaching behind\",\n",
    "    8: \"hair and makeup\",\n",
    "    9: \"talking to passenger\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a path and returns a list of files and matching labels\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    driver_files = np.array(data['filenames'])\n",
    "    driver_targets = np_utils.to_categorical(np.array(data['target']), 10)\n",
    "    return driver_files, driver_targets\n",
    "\n",
    "# Load all of the labeled data, which is in the train folder. This\n",
    "# will be split into separate sets later, so call it all_* for now.\n",
    "all_files, all_targets = load_dataset('StateFarm/imgs/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df = pd.read_csv('StateFarm/driver_imgs_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = file_df.subject.unique()\n",
    "print('There are %d subjects with data'% len(subjects))\n",
    "\n",
    "# Split the files into classes based on subject id. \n",
    "train_subjects = subjects[:-4]\n",
    "validate_subjects = subjects[-4:-3]\n",
    "test_subjects = subjects[-3:]\n",
    "\n",
    "# Print out the subject id numbers for each class\n",
    "print('Training subjects:')\n",
    "print(train_subjects)\n",
    "print('Validate subjects:')\n",
    "print(validate_subjects)\n",
    "print('Test subjects:')\n",
    "print(test_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the rows of the dataframe where the subject is in the appropriate set\n",
    "test_rows = file_df.loc[file_df['subject'].isin(test_subjects)]\n",
    "validate_rows = file_df.loc[file_df['subject'].isin(validate_subjects)]\n",
    "train_rows = file_df.loc[file_df['subject'].isin(train_subjects)]\n",
    "\n",
    "# Generate file names based on the extracted rows\n",
    "train_files = train_rows.apply(lambda x: \"{}/{}\".format(x[1],x[2]), axis=1)\n",
    "validate_files = validate_rows.apply(lambda x: \"{}/{}\".format(x[1],x[2]), axis=1)\n",
    "test_files = test_rows.apply(lambda x: \"{}/{}\".format(x[1],x[2]), axis=1)\n",
    "\n",
    "print(\"train_files count: {}\".format(len(train_files)))\n",
    "print(\"validate_files count: {}\".format(len(validate_files)))\n",
    "print(\"test_files count: {}\".format(len(test_files)))\n",
    "\n",
    "train_classes = train_files.apply(lambda x: x.split('/')[0].split('c')[1])\n",
    "train_classes_categories = np_utils.to_categorical(train_classes)\n",
    "validate_classes = validate_files.apply(lambda x: x.split('/')[0].split('c')[1])\n",
    "validate_classes_categories = np_utils.to_categorical(validate_classes)\n",
    "test_classes = test_files.apply(lambda x: x.split('/')[0].split('c')[1])\n",
    "test_classes_categories = np_utils.to_categorical(test_classes)\n",
    "\n",
    "# Assert that various lengths that should be equal to detect obvious problems \n",
    "# where the wrong variable was used.\n",
    "assert len(train_classes) == len(train_files)\n",
    "assert len(train_classes) == len(train_classes_categories)\n",
    "\n",
    "assert len(validate_classes) == len(validate_files)\n",
    "assert len(validate_classes) == len(validate_classes_categories)\n",
    "\n",
    "assert len(test_classes) == len(test_files)\n",
    "assert len(test_classes) == len(test_classes_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(\"{}/{}\".format('StateFarm/imgs/train', img_path), target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "# Normalize the image to the expected 0->1 values for pretrained networks\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "validate_tensors = paths_to_tensor(validate_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255\n",
    "\n",
    "train_classes_categories = train_classes_categories\n",
    "validate_classes_categories = validate_classes_categories\n",
    "test_classes_categories = test_classes_categories\n",
    "\n",
    "# Assert that the tensor count matches the file count to catch errors where\n",
    "# the wrong variable was used to generate the tensors\n",
    "assert len(train_files) == len(train_tensors)\n",
    "assert len(validate_files) == len(validate_tensors)\n",
    "assert len(test_files) == len(test_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.argmax(train_classes_categories, axis=1))\n",
    "plt.title('Training Class Distribution')\n",
    "plt.xlabel('Image Count')\n",
    "plt.ylabel('Driving Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_class_sample(tcc, tensors):\n",
    "    fig = plt.figure(figsize=(50, 50))  # width, height in inches\n",
    "    dim = 4\n",
    "    for target_class in range(10):\n",
    "        c = np.argmax(tcc, axis=1)\n",
    "        idx = np.where(c == target_class)[0][0]\n",
    "        tensor = tensors[idx]\n",
    "        sub = fig.add_subplot(dim, dim, target_class + 1)\n",
    "        sub.imshow(tensor, interpolation='nearest')\n",
    "        text_params = {'fontweight': 'bold'}\n",
    "\n",
    "        sub.text(0,\n",
    "                 10,\n",
    "                 \"actual: {}\".format(driving_class_to_description[target_class]),\n",
    "                 color='g',\n",
    "                 size=20,\n",
    "                 bbox=dict(boxstyle=\"square\", ec=(1., 0.5, 0.5), fc=(1., 0.8, 0.8), ),\n",
    "                 **text_params)\n",
    "    plt.savefig('analysis/sample.png', format='png')\n",
    "\n",
    "# Print out a sample of each class from the training tensors to understand the classes\n",
    "show_class_sample(train_classes_categories, train_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8\n",
    "# https://www.safaribooksonline.com/library/view/python-deep-learning/9781787125193/a0b05e70-9f53-404c-a975-09ac766389a1.xhtml\n",
    "# https://alexisbcook.github.io/2017/using-transfer-learning-to-classify-images-with-keras/\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "# nb_train_samples = len(train_tensors)\n",
    "# nb_validation_samples = len(validate_tensors) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_params_fast():\n",
    "    return 75, 1\n",
    "\n",
    "def train_params_full():\n",
    "    return 20, 20\n",
    "\n",
    "def optimizer_1(model):\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    opt=\"SGD\"\n",
    "    model.compile(loss = \"categorical_crossentropy\",\n",
    "                  optimizer = optimizers.SGD(lr=lr, momentum=momentum),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return \"opt={},lr={},momentum={}\".format(opt,lr, momentum)\n",
    "\n",
    "def optimizer_2(model):\n",
    "    lr=\"default\"\n",
    "    momentum=\"default\"\n",
    "    opt=\"Adam\"\n",
    "    model.compile(loss = \"categorical_crossentropy\",\n",
    "                  optimizer = optimizers.Adam(),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return \"opt={},lr={},momentum={}\".format(opt,lr, momentum)\n",
    "\n",
    "def model_v1():\n",
    "    pretrain_model = applications.ResNet50(weights = \"imagenet\", \n",
    "                                           include_top=False, \n",
    "                                           input_shape = (img_width, img_height, 3))\n",
    "    train_layers = 0\n",
    "    for layer in pretrain_model.layers[:-train_layers]:\n",
    "        layer.trainable = False\n",
    "    x = pretrain_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(input = pretrain_model.input, output = predictions)\n",
    "    return model, \"resnet50-flat-drop0.5\"\n",
    "\n",
    "def model_v2():\n",
    "    pretrain_model = applications.ResNet50(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "    train_layers = 0\n",
    "    for layer in pretrain_model.layers[:-train_layers]:\n",
    "        layer.trainable = False\n",
    "    x = pretrain_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    predictions = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(input = pretrain_model.input, output = predictions)\n",
    "    return model, \"resnet50-flat-d1024-drop0.5-dense1024\"\n",
    "\n",
    "def model_v3():\n",
    "    pretrain_model = applications.ResNet50(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "    train_layers = 0\n",
    "    for layer in pretrain_model.layers[:-train_layers]:\n",
    "        layer.trainable = False\n",
    "    x = pretrain_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(input = pretrain_model.input, output = predictions)\n",
    "    return model, \"resnet50-model-v3\"\n",
    "\n",
    "def model_v4():\n",
    "    pretrain_model = applications.ResNet50(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "    train_layers = 0\n",
    "    for layer in pretrain_model.layers[:-train_layers]:\n",
    "        layer.trainable = False\n",
    "    x = pretrain_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(input = pretrain_model.input, output = predictions)\n",
    "    return model, \"resnet50-model-v4\"\n",
    "\n",
    "def model_v5():\n",
    "    pretrain_model = applications.ResNet50(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "    train_layers = 0\n",
    "    for layer in pretrain_model.layers[:-train_layers]:\n",
    "        layer.trainable = False\n",
    "    x = pretrain_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(input = pretrain_model.input, output = predictions)\n",
    "    return model, \"resnet50-model-v5\"\n",
    "\n",
    "def model_v6():\n",
    "    pretrain_model = applications.ResNet50(weights = \"imagenet\", \n",
    "                                           include_top=False, \n",
    "                                           pooling='avg',\n",
    "                                           input_shape = (img_width, img_height, 3))\n",
    "    train_layers = 0\n",
    "    for layer in pretrain_model.layers[:-train_layers]:\n",
    "        layer.trainable = False\n",
    "    x = pretrain_model.output\n",
    "    # x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(input = pretrain_model.input, output = predictions)\n",
    "    return model, \"resnet50-model-v6\"\n",
    "\n",
    "def model_v7():\n",
    "    pretrain_model = applications.ResNet50(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "    train_layers = 0\n",
    "    for layer in pretrain_model.layers[:-train_layers]:\n",
    "        layer.trainable = False\n",
    "    x = pretrain_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(input = pretrain_model.input, output = predictions)\n",
    "    return model, \"resnet50-model-v7\"\n",
    "\n",
    "\n",
    "# Experiments are tracked by having immutable functions that define the\n",
    "# model, optimizer, and training parameters. The model_desc string describes\n",
    "# the experiment and is used when generating the graph titles and filenames\n",
    "# so the exact experiment can be located using the output files.\n",
    "batch_size, epochs = train_params_full()\n",
    "model_final, model_name = model_v5()\n",
    "model_optimizer_text = optimizer_2(model_final)\n",
    "\n",
    "model_desc = \"batch_size={},epochs={},{},{}\".format(batch_size, epochs, model_name, model_optimizer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_desc)\n",
    "model_final.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train():\n",
    "    # Checkpoint the model so we can restore it later. If the model \n",
    "    # degrades with additional training epochs we can restore the \n",
    "    # weights with the lowest log loss.\n",
    "    checkpoint = ModelCheckpoint(\"weights/{}.h5\".format(model_desc), \n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=1, \n",
    "                                 save_best_only=True, \n",
    "                                 save_weights_only=False, \n",
    "                                 mode='auto', \n",
    "                                 period=1)\n",
    "    # Stop if the model doesn't improve in 10 epochs to save training time\n",
    "    early = EarlyStopping(monitor='val_loss', \n",
    "                          min_delta=0, \n",
    "                          patience=10, \n",
    "                          verbose=1, \n",
    "                          mode='auto')\n",
    "\n",
    "    history = model_final.fit(x=train_tensors,\n",
    "                              y=train_classes_categories,\n",
    "                              batch_size=batch_size,\n",
    "                              epochs=epochs,\n",
    "                              verbose=1,\n",
    "                              validation_data=(validate_tensors, validate_classes_categories),\n",
    "                              callbacks = [checkpoint, early]\n",
    "                              )\n",
    "    return history\n",
    "history = None\n",
    "# Comment this out if prior weights have been computed for this model\n",
    "# Useful to modify analysis code without rerunning training\n",
    "\n",
    "history = do_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train and validate accuracy for each training epoch, saving \n",
    "# it to a file in the analysis directory to allow experiment results to \n",
    "# be preserved.\n",
    "def generate_accuracy_fig(hist, desc):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(np.arange(len(hist.history['acc'])), hist.history['acc'], label='training')\n",
    "    plt.plot(np.arange(len(hist.history['val_acc'])), hist.history['val_acc'], label='validation')\n",
    "    plt.title('Accuracy\\n {}'.format(desc))\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy ')\n",
    "    plt.legend(loc=0)\n",
    "    plt.savefig('analysis/acc{}.png'.format(desc), format='png')\n",
    "    plt.show()\n",
    "if history is not None:\n",
    "    generate_accuracy_fig(history, model_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train and validate log loss for each training epoch, saving \n",
    "# it to a file in the analysis directory to allow experiment results to \n",
    "# be preserved.\n",
    "def generate_loss_fig(hist, desc):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(np.arange(len(hist.history['loss'])), hist.history['loss'], label='loss')\n",
    "    plt.plot(np.arange(len(hist.history['val_loss'])), hist.history['val_loss'], label='val_loss')\n",
    "    plt.title('Loss\\n {}'.format(desc))\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss ')\n",
    "    plt.legend(loc=0)\n",
    "    plt.savefig('analysis/loss{}.png'.format(desc), format='png')\n",
    "    plt.show()\n",
    "if history is not None:\n",
    "    generate_loss_fig(history, model_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best weights found during the experiment instead of leaving\n",
    "# the final weights from the last training epoch.\n",
    "model_final.load_weights(\"weights/{}.h5\".format(model_desc))\n",
    "\n",
    "# Generate test predictions using the best weights for the \n",
    "# experiment\n",
    "test_predictions = model_final.predict(x=test_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_metrics(pred, actual):\n",
    "    actual_classes = np.argmax(actual, axis=1)\n",
    "    predicted_classes = np.argmax(pred, axis=1)\n",
    "\n",
    "    test_accuracy = 100*np.sum(actual_classes==predicted_classes)/len(actual)\n",
    "    loss = log_loss(y_pred=pred, y_true=actual)\n",
    "\n",
    "    return \"Test accuracy: %.4f%%\\nLog loss: %f\" % (test_accuracy, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the metrics from the test set to a file in the analysis directory to \n",
    "# allow experiment results to be preserved.\n",
    "test_results = prediction_metrics(test_predictions, test_classes_categories)\n",
    "with open(\"analysis/test-results{}.txt\".format(model_desc), \"w\") as text_file:\n",
    "    text_file.write(test_results)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the naive classifier that always picks safe driving\n",
    "pred_attentive = [1.0] + [0] * 9\n",
    "benchmark_all_pred_attentive = [pred_attentive] * len(test_classes_categories)\n",
    "\n",
    "print(prediction_metrics(benchmark_all_pred_attentive, test_classes_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show a sampling of misclassified images with label annotations showing \n",
    "# the true and predicted classes. This data can inform an understanding of\n",
    "# why misclassifications might happen.\n",
    "def generate_mispredict_fig():\n",
    "    predicted_classes = np.argmax(test_predictions, axis=1)\n",
    "    actual_classes = np.argmax(test_classes_categories, axis=1)\n",
    "    fig = plt.figure(figsize=(50, 50))  # width, height in inches\n",
    "    j = 0\n",
    "    dim = 4\n",
    "    for i in shuffle(range(len(test_tensors))):\n",
    "        tensor = test_tensors[i]\n",
    "        if predicted_classes[i] != actual_classes[i]:\n",
    "            sub = fig.add_subplot(dim, dim, j + 1)\n",
    "            sub.imshow(tensor, interpolation='nearest')\n",
    "            text_params = {'fontweight': 'bold'}\n",
    "            sub.text(0,\n",
    "                     10,\n",
    "                     \"pred: {}\".format(driving_class_to_description[predicted_classes[i]]),\n",
    "                     color='r',\n",
    "                     size=20,\n",
    "                     bbox=dict(boxstyle=\"square\", ec=(1., 0.5, 0.5), fc=(1., 0.8, 0.8), ),\n",
    "                     **text_params)\n",
    "            sub.text(0, \n",
    "                     25, \n",
    "                     \"actual: {}\".format(driving_class_to_description[actual_classes[i]]), \n",
    "                     color='g',\n",
    "                     size=20,\n",
    "                     bbox=dict(boxstyle=\"square\", ec=(1., 0.5, 0.5), fc=(1., 0.8, 0.8), ),\n",
    "                     **text_params)\n",
    "\n",
    "            j = j + 1\n",
    "            if j == dim * dim:\n",
    "                break\n",
    "    plt.savefig('analysis/mispredicts.png', format='png')\n",
    "generate_mispredict_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a confusion matrix to provide a high level summary\n",
    "# of where mispedictions are happening. \n",
    "def generate_confusion_fig():   \n",
    "    predicted_classes = np.argmax(test_predictions, axis=1)\n",
    "    actual_classes = np.argmax(test_classes_categories, axis=1)\n",
    "    \n",
    "    confusion = confusion_matrix(actual_classes, predicted_classes)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    hm = sns.heatmap(confusion, annot=True, fmt=\"d\")\n",
    "    hm.set_ylabel('True label')\n",
    "    hm.set_xlabel('Predicted label')\n",
    "    hm.set_title('Confusion Matrix\\n{}'.format(model_desc))\n",
    "    plt.savefig('analysis/confusion{}.png'.format(model_desc), format='png')\n",
    "generate_confusion_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
